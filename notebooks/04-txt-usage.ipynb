{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS TXT records analysis\n",
    "In this notebook we'll take a look into TXT records statistics. We'll use data generated by shell script located in `src/txt_analysis/monthly_statistics.sh`. To get the data in a usable form, we download it first from HDFS by issuing following command:\n",
    "```\n",
    "hdfs dfs -copyToLocal 201*results.txt ~\n",
    "```\n",
    "Then we just run `src/combine.sh` script.\n",
    "We now have `201YMMDD_results.json` files that we can use for presentation.\n",
    "We put them in `data/txt_analysis_results` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/omar/code/mako/notebooks/../data/txt_analysis_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-01e9a8b64b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdata_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/omar/code/mako/notebooks/../data/txt_analysis_results'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clrs\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "file_location = (\n",
    "    os.path.normcase(\n",
    "        os.path.join(os.getcwd(), \"../data/txt_analysis_results\")\n",
    "    )\n",
    ")\n",
    "\n",
    "data_points = []\n",
    "for filename in os.listdir(file_location):\n",
    "    with open(os.path.join(file_location, filename), \"r\") as file:\n",
    "        data_points.append(eval(file.read()))\n",
    "        \n",
    "def convert_LD_to_DL(dictionaries):\n",
    "    dl = {}\n",
    "    for key, value in dictionaries[0].items():\n",
    "        if type(value) is not dict:\n",
    "            dl[key] = get_summary_for_from(key, dictionaries)\n",
    "        else:\n",
    "            dl[key] = convert_LD_to_DL([entry[key] for entry in dictionaries])\n",
    "    return dl\n",
    "\n",
    "def get_summary_for_from(key, list_of_dictionaries):\n",
    "    summary = np.array([])\n",
    "    for result in list_of_dictionaries:\n",
    "        summary = np.append(summary, result[key])\n",
    "    return summary\n",
    "\n",
    "def sum_over_children(dictionary):\n",
    "    summary = np.zeros(len(data_points))\n",
    "    for key, value in dictionary.items():\n",
    "        if type(value) is not dict:\n",
    "            summary += value\n",
    "        else:\n",
    "            summary += sum_over_children(value)\n",
    "    return summary\n",
    "\n",
    "def sum_over_children_single(dictionary):\n",
    "    summary = 0\n",
    "    for key, value in dictionary.items():\n",
    "        if type(value) is not dict:\n",
    "            summary += value\n",
    "        else:\n",
    "            summary += sum_over_children_single(value)\n",
    "    return summary\n",
    "    \n",
    "summary = convert_LD_to_DL(data_points)\n",
    "labels = summary[\"day\"]\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, points, color, info):\n",
    "        self.points = points\n",
    "        self.color = color\n",
    "        self.info = info\n",
    "\n",
    "def make_a_time_graph(title, data, labels=labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    prepare_basic_line_graph(title)\n",
    "    for entry in data:\n",
    "        plt.plot(labels, entry.points, entry.color, label=entry.info)\n",
    "    ax.legend(loc='center right', shadow=True, bbox_to_anchor=(1.2, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "def prepare_basic_line_graph(title):\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 12\n",
    "    fig_size[1] = 8\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.xlabel('Month')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.ylabel('Number')\n",
    "    plt.grid()\n",
    "    plt.title(title)\n",
    "    \n",
    "def make_a_pie_chart(title, values, labels):\n",
    "    plt.title(title)\n",
    "    plt.pie(values, labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TXT records in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prepare_basic_line_graph('Number of records in researched months')\n",
    "plt.plot(labels, summary[\"total\"], color='blue')\n",
    "plt.plot(labels, summary[\"txt\"], color='purple')\n",
    "plt.plot(labels, summary[\"correct_txt\"], color='green')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "percentages = summary[\"txt\"] / summary[\"total\"]\n",
    "print(\"Highest percentage: {:.2%}\".format(percentages.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, TXT records make up only a small percentage of all records.\n",
    "Let's look at incorrect values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [Line(summary[\"txt\"] - summary[\"correct_txt\"], 'red', \"Empty records\")]\n",
    "make_a_time_graph('Number of empty TXT records in researched months', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usages\n",
    "Let's see now, what are the most common uses for TXT records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "itr = 0\n",
    "for key, value in summary[\"specific\"].items():\n",
    "    if type(value) is dict:\n",
    "        lines.append(Line(sum_over_children(value), 'C'+str(itr), key))\n",
    "    else:\n",
    "        lines.append(Line(value, 'C' + str(itr), key))\n",
    "    itr += 1\n",
    "make_a_time_graph('Different usages of TXT records', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "labels = []\n",
    "for key, value in data_points[-1][\"specific\"].items():\n",
    "    labels.append(key)\n",
    "    if type(value) is dict:\n",
    "        values.append(sum_over_children_single(value))\n",
    "    else:\n",
    "        values.append(value)\n",
    "values, labels = (list(t) for t in zip(*sorted(zip(values, labels))))\n",
    "\n",
    "labels.append(\"other\")\n",
    "values.append(data_points[-1][\"correct_txt\"] - sum(values))\n",
    "make_a_pie_chart('Different usages of TXT records for 2017-12-01', values, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, mail security, domain identifiers and just keys make up most of the collected TXT records. Let's see the components of mail security:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (data_points[-1][\"specific\"][\"mail-security\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting a pie chart makes no sense here, obviously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain identifiers\n",
    "The big part of TXT records is domain identifiers group. The proportions in this group look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "labels = []\n",
    "itr = 0\n",
    "for key, value in data_points[-1][\"specific\"][\"domain-identifiers\"].items():\n",
    "    labels.append(key)\n",
    "    values.append(value)\n",
    "    itr += 1\n",
    "values, labels = (list(t) for t in zip(*sorted(zip(values, labels))))\n",
    "\n",
    "make_a_pie_chart('Different usages of TXT records for 2017-12-01', values, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original question - are there any sensitive data?\n",
    "The question proved to more difficult, than it seems.\n",
    "\n",
    "There are no papers on this topic (automatic sensitive data recognition), so we came up with something as simple as possible - just search for string such as \"password\", \"pswd\" etc.\n",
    "And, surprisingly, we found a few things, such as:\n",
    "<img src=\"assets/private_key.jpeg\"/>\n",
    "Or, partially sensitive:\n",
    "<img src=\"assets/login.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other records\n",
    "The records we failed to assign were mostly configuration errors such as:\n",
    "<img src=\"assets/wrong_conf.png\"/>\n",
    "<img src=\"assets/wrong_conf1.png\"/>\n",
    "<img src=\"assets/wrong_conf2.png\"/>\n",
    "<img src=\"assets/funny.png\"/>\n",
    "\n",
    "There were also other domain identification records, some of them not popular enough to make a difference:\n",
    "<img src=\"assets/other.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
